{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f97f4dd",
   "metadata": {},
   "source": [
    "### Scrapping data from Web site\n",
    "\n",
    "\n",
    " in this jupyter notebook, data from a website were extracted using python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8851f2",
   "metadata": {},
   "source": [
    "----------Importing Nessary packages and libraries----------- <br>\n",
    "Request package use to connect programme with the internet <br>\n",
    "Beautifulsoup is a package that use to deal with the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e43e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540305f9",
   "metadata": {},
   "source": [
    "Making the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f87d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://xpress.jobs/jobs\")\n",
    "c=r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b34005",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(c,\"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e3d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all=soup.find_all(\"li\",{\"class\":\"job_listing\"})\n",
    "# all[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdf34e",
   "metadata": {},
   "source": [
    "extact a one label (job title) for demonstrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d332c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deputy Manager - Internal Audit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all[0].find(\"h3\",{\"class\":\"job_listing-title\"}).text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b62f88",
   "metadata": {},
   "source": [
    "Extract the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f609fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in all:\n",
    "    #print (item.find(\"h3\",{\"class\":\"job_listing-title\"}).text)\n",
    "    #print (item.find(\"strong\").text)\n",
    "    #print (item.find(\"div\",{\"class\":\"location\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "    #print (item.find(\"li\",{\"class\":\"job-type\"}).text)\n",
    "    #print (item.find(\"li\",{\"class\":\"date\"}).text.replace(\"Expires on \",\"\"))\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4602d56",
   "metadata": {},
   "source": [
    "Add the data to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for item in all:\n",
    "    d={}\n",
    "    d[\"Title\"]=item.find(\"h3\",{\"class\":\"job_listing-title\"}).text\n",
    "    d[\"Employee\"]=item.find(\"strong\").text\n",
    "    d[\"Location\"]=item.find(\"div\",{\"class\":\"location\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "    d[\"type\"]=item.find(\"li\",{\"class\":\"job-type\"}).text\n",
    "    d[\"Expire_date\"]=item.find(\"li\",{\"class\":\"date\"}).text.replace(\"Expires on \",\"\")\n",
    "    l.append(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ab2b5",
   "metadata": {},
   "source": [
    "Add data into panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4fe4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df=pandas.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04be18ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Employee</th>\n",
       "      <th>Location</th>\n",
       "      <th>type</th>\n",
       "      <th>Expire_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Software Engineer – Full Stack</td>\n",
       "      <td>ONEPAY</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>25/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enrolment Executive</td>\n",
       "      <td>VIEC Sri Lanka</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>25/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Software Engineer - Full Stack</td>\n",
       "      <td>ONEPAY</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>25/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crew Member - (Full Time) - Minuwangoda</td>\n",
       "      <td>Domino's Pizza Sri Lanka</td>\n",
       "      <td>Gampaha</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>25/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auditors</td>\n",
       "      <td>Ernst &amp; Young</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Urgent</td>\n",
       "      <td>17/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERP Consultant - Internship</td>\n",
       "      <td>Ernst &amp; Young</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Urgent</td>\n",
       "      <td>17/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Human Resource Manager</td>\n",
       "      <td>Axis BPO</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>16/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Junior Operations Assistant (Female)</td>\n",
       "      <td>XpressJobs</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>17/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crew Member - (Full Time) - Havelock</td>\n",
       "      <td>Domino's Pizza Sri Lanka</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>25/05/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sales Executive - Fixed Deposits | AMW Capital...</td>\n",
       "      <td>AMW Capital Leasing &amp; Finance PLC</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>Urgent</td>\n",
       "      <td>20/05/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0              Senior Software Engineer – Full Stack   \n",
       "1                               Enrolment Executive    \n",
       "2                     Software Engineer - Full Stack   \n",
       "3            Crew Member - (Full Time) - Minuwangoda   \n",
       "4                                          Auditors    \n",
       "5                        ERP Consultant - Internship   \n",
       "6                             Human Resource Manager   \n",
       "7              Junior Operations Assistant (Female)    \n",
       "8               Crew Member - (Full Time) - Havelock   \n",
       "9  Sales Executive - Fixed Deposits | AMW Capital...   \n",
       "\n",
       "                            Employee Location       type  Expire_date  \n",
       "0                             ONEPAY  Colombo  Full-Time  25/05/2022   \n",
       "1                     VIEC Sri Lanka  Colombo  Full-Time  25/05/2022   \n",
       "2                             ONEPAY  Colombo  Full-Time  25/05/2022   \n",
       "3           Domino's Pizza Sri Lanka  Gampaha  Full-Time  25/05/2022   \n",
       "4                      Ernst & Young  Colombo     Urgent  17/05/2022   \n",
       "5                      Ernst & Young  Colombo     Urgent  17/05/2022   \n",
       "6                           Axis BPO  Colombo  Full-Time  16/05/2022   \n",
       "7                        XpressJobs   Colombo  Full-Time  17/05/2022   \n",
       "8           Domino's Pizza Sri Lanka  Colombo  Full-Time  25/05/2022   \n",
       "9  AMW Capital Leasing & Finance PLC  Colombo     Urgent  20/05/2022   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5963ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "base_url=\"https://xpress.jobs/jobs?page=\"\n",
    "for urls in range(1,222):\n",
    "    url=base_url+str(urls)\n",
    "    \n",
    "    r=requests.get(url)\n",
    "    c=r.content\n",
    "    soup=BeautifulSoup(c,\"html.parser\")\n",
    "    \n",
    "    all=soup.find_all(\"li\",{\"class\":\"job_listing\"})\n",
    "    for item in all:\n",
    "        d={}\n",
    "        d[\"Title\"]=item.find(\"h3\",{\"class\":\"job_listing-title\"}).text\n",
    "        d[\"Employee\"]=item.find(\"strong\").text\n",
    "        d[\"Location\"]=item.find(\"div\",{\"class\":\"location\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "        d[\"type\"]=item.find(\"li\",{\"class\":\"job-type\"}).text\n",
    "        d[\"Expire_date\"]=item.find(\"li\",{\"class\":\"date\"}).text.replace(\"Expires on \",\"\")\n",
    "        l.append(d)\n",
    "\n",
    "import pandas\n",
    "all_job=pandas.DataFrame(l)\n",
    "\n",
    "\n",
    "all_job.to_csv(\"all_job.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7693d3",
   "metadata": {},
   "source": [
    "save all jobs listing to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job.to_csv(\"all_job.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb74ff1",
   "metadata": {},
   "source": [
    "#### Extracting job catogries from web site to add aditional features to the data\n",
    "jobs are not categorised in first phase in the web site hence separate tables are created under each job catogery.\n",
    "\n",
    "\n",
    "In this section i used another web scraping methods using the selenium web driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4800cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "\n",
    "\n",
    "driver= webdriver.Chrome(\"C:/chrome driver/chromedriver.exe\")\n",
    "driver.get(\"https://xpress.jobs/Jobs/sector\")\n",
    "elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "\n",
    "\n",
    "links_for_job_categories=[]\n",
    "for elem in elems:\n",
    "   after_re=re.findall(\"^https://xpress.jobs/Jobs/Sector/.+\", elem.get_attribute(\"href\"))\n",
    "   #print(after_re)\n",
    "   \n",
    "   links_for_job_categories.append(after_re)\n",
    "   links_for_job_categories = list(filter(None, links_for_job_categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5630496",
   "metadata": {},
   "source": [
    "In the next section data is categorized and export in to respective csv files.Also combined dataframe was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97b3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete accounting\n",
      "complete administration\n",
      "complete agriculture\n",
      "complete apparel\n",
      "complete automobile\n",
      "complete banking\n",
      "complete bpo-kpo\n",
      "complete construction\n",
      "complete business-management\n",
      "complete customer-service\n",
      "complete delivery-driving-transport\n",
      "complete education\n",
      "complete electronics\n",
      "complete engineering\n",
      "complete environment\n",
      "complete fmcg\n",
      "complete foreign-jobs\n",
      "complete government\n",
      "complete healthcare\n",
      "complete hospitality\n",
      "complete hr\n",
      "complete insurance\n",
      "complete interior-design\n",
      "complete internship\n",
      "complete it-hware\n",
      "complete it\n",
      "complete legal\n",
      "complete media\n",
      "complete pharmaceutical\n",
      "complete production-operations\n",
      "complete project-management\n",
      "complete quality-assurance\n",
      "complete recoveries\n",
      "complete retail\n",
      "complete sales\n",
      "complete school-leavers\n",
      "complete science\n",
      "complete security\n",
      "complete senior-management\n",
      "complete startup\n",
      "complete supply-chain-procurement\n",
      "complete technical\n",
      "complete telecommunications\n",
      "complete training-development\n",
      "complete travel\n",
      "complete walkin\n",
      "complete remote\n",
      "complete other\n",
      "                                                 Title  \\\n",
      "0                      Deputy Manager - Internal Audit   \n",
      "1                                   Accounts Executive   \n",
      "2                                    Finance Assistant   \n",
      "3                                          Coordinator   \n",
      "4                                      Audit Executive   \n",
      "..                                                 ...   \n",
      "175                            Fire and Safety Officer   \n",
      "176         වායු සමීකරන තාක්ෂණ ශිල්පී (A/C Technician)   \n",
      "177  රීච් ට්‍රක් රථ ක්‍රියාකරු | Reach Truck Operat...   \n",
      "178  ජලනල කාර්මික ශිල්පී / පින්තාරු ශිල්පී - Plumbe...   \n",
      "179               ගබඩා සහයක - කෙරවලපිටිය (දිවා/රාත්රී)   \n",
      "\n",
      "                    Employee Location       type  Expire_date     catogry  \n",
      "0         Nations Trust Bank  Colombo  Full-Time  30/05/2022   accounting  \n",
      "1    SPC Consultants Pvt Ltd  Colombo  Full-Time  30/05/2022   accounting  \n",
      "2        Pizza Hut Sri Lanka  Colombo  Full-Time  30/05/2022   accounting  \n",
      "3                  Abans PLC  Colombo  Full-Time  30/05/2022   accounting  \n",
      "4                  Abans PLC  Colombo  Full-Time  30/05/2022   accounting  \n",
      "..                       ...      ...        ...          ...         ...  \n",
      "175   Nawaloka Hospitals PLC  Colombo  Full-Time  20/05/2022        other  \n",
      "176                 ODEL PLC  Colombo  Full-Time  25/05/2022        other  \n",
      "177                  Keells   Colombo  Full-Time  20/05/2022        other  \n",
      "178                 ODEL PLC  Colombo  Full-Time  25/05/2022        other  \n",
      "179                  Keells   Colombo  Full-Time  20/05/2022        other  \n",
      "\n",
      "[7680 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def xtracting(final_url):\n",
    "    r=requests.get(final_url)\n",
    "    c=r.content\n",
    "    soup=BeautifulSoup(c,\"html.parser\")\n",
    "    all=soup.find_all(\"li\",{\"class\":\"job_listing\"})\n",
    "    for item in all:\n",
    "        li={}\n",
    "        li[\"Title\"]=item.find(\"h3\",{\"class\":\"job_listing-title\"}).text\n",
    "        li[\"Employee\"]=item.find(\"strong\").text\n",
    "        li[\"Location\"]=item.find(\"div\",{\"class\":\"location\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "        li[\"type\"]=item.find(\"li\",{\"class\":\"job-type\"}).text\n",
    "        li[\"Expire_date\"]=item.find(\"li\",{\"class\":\"date\"}).text.replace(\"Expires on \",\"\")\n",
    "        li[\"catogry\"]=urldd.partition(\"Sector/\")[2]\n",
    "        lo.append(li)\n",
    "        \n",
    "    return lo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "driver= webdriver.Chrome(\"C:/chrome driver/chromedriver.exe\")\n",
    "column_names = [\"Title\", \"Employee\", \"Location\", \"type\", \"Expire_date\", \"catogry\"]\n",
    "all_data=pd.DataFrame(columns = column_names)\n",
    "for url_o in links_for_job_categories:\n",
    "    title_list=[]\n",
    "    \n",
    "    for urldd in url_o:\n",
    "        #print(urldd)\n",
    "        title=urldd.partition(\"Sector/\")[2]\n",
    "\n",
    "        #generate re occuring link for multiple links\n",
    "        driver.get(urldd)\n",
    "        elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "        pages=[]\n",
    "        for elem in elems:\n",
    "            #print(elem.get_attribute(\"href\"))\n",
    "            pg_links=elem.get_attribute(\"href\")\n",
    "            pg_links2=re.findall(\"^https://xpress.jobs/jobs\\?page=.+\",pg_links)\n",
    "            pages.append(pg_links2)\n",
    "            pages=list(filter(None, pages))\n",
    "\n",
    "        try:\n",
    "            splited_url=' '.join(map(str, pages[0]))\n",
    "            splited_url=splited_url.split('2&')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #print(splited_url)\n",
    "\n",
    "\n",
    "        #find no of pages\n",
    "        lo=[]\n",
    "        r=requests.get(urldd)\n",
    "        c=r.content\n",
    "        soup=BeautifulSoup(c,\"html.parser\")\n",
    "        t=soup.find_all(\"div\",{\"class\":\"paginate-links\"})\n",
    "        #print(t[0])\n",
    "        for i in t:\n",
    "            try:\n",
    "                no_of_pages=i.find_all(\"a\",{\"class\":\"page-numbers\"})[-2].text\n",
    "                #print(no_of_pages)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        #extractng\n",
    "        if int(no_of_pages) >1:\n",
    "            for i in range(int(no_of_pages)):\n",
    "                final_url=splited_url[0]+str(i)+\"&\"+splited_url[1]\n",
    "                xtracting(final_url)\n",
    "            \n",
    "            titl=pd.DataFrame(lo)\n",
    "            titl.to_csv(title +\".csv\")\n",
    "            #print(titl)\n",
    "            print(\"complete \"+ title)\n",
    "                \n",
    "\n",
    "        else:\n",
    "            final_url=urldd\n",
    "            xtracting(final_url)\n",
    "            titl=pd.DataFrame(lo)\n",
    "            titl.to_csv(title +\".csv\")\n",
    "            print(titl)\n",
    "            print(\"complete \"+ title)\n",
    "    \n",
    "    df=[all_data,titl]\n",
    "    all_data=pd.concat(df)\n",
    "    \n",
    "    all_data.to_csv(\"final.csv\")\n",
    "\n",
    "print(all_data)     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2616aab",
   "metadata": {},
   "source": [
    "jobs for individual categories are extracted seperately.\n",
    "two main csv files whichtaken from using BeautifulSup and Selenium Driver stored sepately.\n",
    "Databases extracted from this are used for further analysis to find extra details.\n",
    "\n",
    "\n",
    "---------DATA ACCUIRE FROM THIS NOTEBOOK IS ONLY FOR EDUCATIONAL PURPOSE----------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3dbf391ff736e1e6d5dcb07453c66342a39d303f6341c6ccec0398b4ff653ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
